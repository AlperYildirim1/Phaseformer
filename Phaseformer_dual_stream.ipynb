{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1zwWKwUf7bu7Uvy9Ls85RXWVFFTHZygj9",
      "authorship_tag": "ABX9TyMfN+qD8ebz1mdP+rScM19a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Phaseformer/blob/main/Phaseformer_dual_stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFU2msecOMVG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PhaseDualTransformer v2 (Hybrid) â€” Training Script\n",
        "==================================================\n",
        "A Dual-Stream Decoder:\n",
        "  1. Sensory Stream (Standard Attention + RoPE)\n",
        "  2. Relational Stream (Phase Interference + Complex RoPE + Gated)\n",
        "\n",
        "Configuration (60M Tier):\n",
        "  - d_model: 512\n",
        "  - Depth: 11\n",
        "  - Split: 4 Sensory Heads / 4 Phase Heads\n",
        "  - Dropout: 0.0\n",
        "  - Phase Gate: init -5.0 (sigmoid â‰ˆ 0.007, slowly opens)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION (60M HYBRID)\n",
        "# ==========================================\n",
        "HF_DATASET    = \"Yujivus/fineweb-edu-phaseformer-250M-v2\"\n",
        "\n",
        "VOCAB_SIZE    = 32768\n",
        "SEQ_LEN       = 512\n",
        "BATCH_SIZE    = 32\n",
        "GRAD_ACCUM    = 4       # Effective batch = 128\n",
        "EPOCHS        = 1\n",
        "LR            = 6e-4\n",
        "D_MODEL       = 512\n",
        "DEPTH         = 11\n",
        "N_SENSORY     = 4\n",
        "N_PHASE       = 4\n",
        "HEAD_DIM      = 64\n",
        "DROPOUT       = 0.0\n",
        "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ROTARY POSITION EMBEDDINGS\n",
        "# ==========================================\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"Standard RoPE for the Sensory Stream.\"\"\"\n",
        "    def __init__(self, dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (max_period ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, seq_len, device):\n",
        "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb.cos(), emb.sin()\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    \"\"\"Apply standard RoPE to real-valued Q, K tensors.\"\"\"\n",
        "    cos = cos.view(1, 1, *cos.shape)\n",
        "    sin = sin.view(1, 1, *sin.shape)\n",
        "    def rotate_half(x):\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "# ==========================================\n",
        "# 3. PARALLEL PHASE ATTENTION\n",
        "# ==========================================\n",
        "\n",
        "class ParallelPhaseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Batched multi-head phase attention.\n",
        "    All heads computed in parallel via single projections.\n",
        "    Q, K constrained to unit circle. Complex RoPE for position.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, head_dim, dropout=0.0, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # Real -> Complex projections (2x for real/imag pairs)\n",
        "        inner_dim = n_heads * head_dim * 2\n",
        "        self.W_q = nn.Linear(d_model, inner_dim, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, inner_dim, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, inner_dim, bias=False)\n",
        "\n",
        "        # Complex -> Real bridge\n",
        "        self.out_proj = nn.Linear(inner_dim, n_heads * head_dim, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Complex RoPE: one frequency per complex dimension\n",
        "        inv_freq = 1.0 / (max_period ** (torch.arange(0, head_dim).float() / head_dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, _ = x.shape\n",
        "\n",
        "        # A. Project real -> complex\n",
        "        q = torch.view_as_complex(self.W_q(x).view(B, L, self.n_heads, self.head_dim, 2))\n",
        "        k = torch.view_as_complex(self.W_k(x).view(B, L, self.n_heads, self.head_dim, 2))\n",
        "        v = torch.view_as_complex(self.W_v(x).view(B, L, self.n_heads, self.head_dim, 2))\n",
        "\n",
        "        # B. Unit circle constraint on Q, K (phase-only inductive bias)\n",
        "        q = q / (q.abs() + 1e-6)\n",
        "        k = k / (k.abs() + 1e-6)\n",
        "\n",
        "        # C. Complex RoPE: multiply by e^(iÏ‰t)\n",
        "        t = torch.arange(L, device=x.device, dtype=self.inv_freq.dtype)\n",
        "        angles = torch.outer(t, self.inv_freq)  # [L, head_dim]\n",
        "        rotors = torch.polar(torch.ones_like(angles), angles).view(1, L, 1, self.head_dim)\n",
        "        q = q * rotors\n",
        "        k = k * rotors\n",
        "\n",
        "        # D. Attention in complex space\n",
        "        q = q.permute(0, 2, 1, 3)  # [B, H, L, D]\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Phase-locking scores: real part of hermitian dot product\n",
        "        scores = (q @ k.transpose(-2, -1).conj()).real / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn.to(v.dtype) @ v\n",
        "\n",
        "        # E. Complex -> Real output\n",
        "        out = out.transpose(1, 2)  # [B, L, H, D]\n",
        "        out_flat = torch.view_as_real(out).flatten(2)  # [B, L, H*D*2]\n",
        "\n",
        "        return self.out_proj(out_flat)\n",
        "\n",
        "# ==========================================\n",
        "# 4. DUAL STREAM ATTENTION (Gated Phase)\n",
        "# ==========================================\n",
        "\n",
        "class DualStreamAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Sensory stream: standard real-valued MHA + RoPE\n",
        "    Phase stream: complex-valued MHA + complex RoPE + learned gate\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_sensory, n_phase, head_dim=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.n_sensory = n_sensory\n",
        "        self.n_phase = n_phase\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # Stream A: Sensory (Standard)\n",
        "        if n_sensory > 0:\n",
        "            self.sensory_qkv = nn.Linear(d_model, 3 * n_sensory * head_dim, bias=False)\n",
        "\n",
        "        # Stream B: Phase (Relational)\n",
        "        if n_phase > 0:\n",
        "            self.phase_attn = ParallelPhaseAttention(d_model, n_phase, head_dim, dropout)\n",
        "            # Gate: starts near-zero (sigmoid(-5) â‰ˆ 0.007), learns to open\n",
        "            self.phase_gate = nn.Parameter(torch.full((1, 1, n_phase * head_dim), -5.0))\n",
        "\n",
        "        # Integration: concat both streams -> d_model\n",
        "        total_dim = (n_sensory + n_phase) * head_dim\n",
        "        self.out_proj = nn.Linear(total_dim, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, sensory_rope_cos, sensory_rope_sin):\n",
        "        streams = []\n",
        "\n",
        "        # 1. Sensory Stream\n",
        "        if self.n_sensory > 0:\n",
        "            B, L, _ = x.shape\n",
        "            qkv = self.sensory_qkv(x).reshape(B, L, 3, self.n_sensory, self.head_dim)\n",
        "            q, k, v = qkv.unbind(dim=2)\n",
        "\n",
        "            # 1. Transpose ALL to [Batch, Heads, Length, Head_Dim]\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            v = v.transpose(1, 2)\n",
        "\n",
        "            # 2. Apply Standard RoPE\n",
        "            q, k = apply_rotary_pos_emb(q, k, sensory_rope_cos, sensory_rope_sin)\n",
        "\n",
        "            # 3. Flash Attention (causal)\n",
        "            out_s = F.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                is_causal=True,\n",
        "                dropout_p=0.0\n",
        "            )\n",
        "            streams.append(out_s.transpose(1, 2).flatten(2))\n",
        "\n",
        "        # 2. Phase Stream (Gated)\n",
        "        if self.n_phase > 0:\n",
        "            phase_out = self.phase_attn(x)\n",
        "            phase_out = torch.sigmoid(self.phase_gate) * phase_out\n",
        "            streams.append(phase_out)\n",
        "\n",
        "        # 3. Fuse\n",
        "        return self.out_proj(torch.cat(streams, dim=-1))\n",
        "\n",
        "# ==========================================\n",
        "# 5. PHASE DUAL TRANSFORMER\n",
        "# ==========================================\n",
        "\n",
        "class PhaseDualTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, depth, n_sensory, n_phase,\n",
        "                 head_dim=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.rotary_emb = RotaryEmbedding(head_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleDict({\n",
        "                \"norm1\": nn.LayerNorm(d_model),\n",
        "                \"attn\": DualStreamAttention(d_model, n_sensory, n_phase, head_dim, dropout),\n",
        "                \"norm2\": nn.LayerNorm(d_model),\n",
        "                \"mlp\": nn.Sequential(\n",
        "                    nn.Linear(d_model, 4 * d_model),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(4 * d_model, d_model),\n",
        "                )\n",
        "            })\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.head.weight = self.token_emb.weight  # Weight tying\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.token_emb(input_ids) * math.sqrt(self.d_model)\n",
        "        cos, sin = self.rotary_emb(x.shape[1], x.device)\n",
        "\n",
        "        for l in self.layers:\n",
        "            x = x + l[\"attn\"](l[\"norm1\"](x), cos, sin)\n",
        "            x = x + l[\"mlp\"](l[\"norm2\"](x))\n",
        "\n",
        "        return self.head(self.norm_f(x))\n",
        "\n",
        "# ==========================================\n",
        "# 6. OPTIMIZER\n",
        "# ==========================================\n",
        "\n",
        "def configure_optimizers(model, learning_rate, weight_decay=0.1, device_type='cuda'):\n",
        "    \"\"\"\n",
        "    Decays: Linear weights, Embeddings.\n",
        "    Protects: Biases, LayerNorms, Gates.\n",
        "    \"\"\"\n",
        "    decay_params = []\n",
        "    nodecay_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        # Protect: biases, norms, gates\n",
        "        if param.ndim < 2 or \"norm\" in name or \"bias\" in name or \"gate\" in name:\n",
        "            nodecay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    decay_count = sum(p.numel() for p in decay_params)\n",
        "    nodecay_count = sum(p.numel() for p in nodecay_params)\n",
        "    print(f\"ðŸ”§ Optimizer: Decay {decay_count/1e6:.1f}M | Protect {nodecay_count/1e3:.1f}K\")\n",
        "\n",
        "    use_fused = (device_type == 'cuda') and \\\n",
        "                ('fused' in torch.optim.AdamW.__init__.__code__.co_varnames)\n",
        "    extra_args = dict(fused=True) if use_fused else dict()\n",
        "\n",
        "    return torch.optim.AdamW(optim_groups, lr=learning_rate, **extra_args)\n",
        "\n",
        "# ==========================================\n",
        "# 7. TRAINING LOOP\n",
        "# ==========================================\n",
        "\n",
        "def collate_causal_lm(batch):\n",
        "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch])\n",
        "    return {\n",
        "        \"input_ids\": input_ids[:, :-1],\n",
        "        \"labels\":    input_ids[:, 1:],\n",
        "    }\n",
        "\n",
        "def run_training():\n",
        "    run_id = hashlib.md5(datetime.now().strftime(\"%Y%m%d%H%M%S\").encode()).hexdigest()[:8]\n",
        "    SAVE_DIR = os.path.join(\"/content/drive/My Drive/PhaseFormer\", f\"Hybrid_60M_{run_id}\")\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir=SAVE_DIR)\n",
        "\n",
        "    # ---- Data ----\n",
        "    print(f\"â¬‡ï¸ Loading dataset from {HF_DATASET}...\")\n",
        "    dataset = load_dataset(HF_DATASET)\n",
        "    train_ds = dataset[\"train\"]\n",
        "    valid_ds = dataset[\"validation\"]\n",
        "    print(f\"   Train: {len(train_ds):,} chunks | Val: {len(valid_ds):,} chunks\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        collate_fn=collate_causal_lm, num_workers=2, pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_ds, batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_causal_lm, num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # ---- Model ----\n",
        "    print(f\"\\nâš¡ HYBRID 60M (d={D_MODEL}, L={DEPTH}, S={N_SENSORY}, P={N_PHASE})\")\n",
        "    model = PhaseDualTransformer(\n",
        "        VOCAB_SIZE, D_MODEL, DEPTH, N_SENSORY, N_PHASE, HEAD_DIM, DROPOUT\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # --- ADD THIS INITIALIZATION BLOCK ---\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    model.apply(_init_weights)\n",
        "    # -------------------------------------\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    # Robust unique count (handles weight tying)\n",
        "    seen = set()\n",
        "    unique_params = sum(p.numel() for p in model.parameters()\n",
        "                        if p.requires_grad and id(p) not in seen and not seen.add(id(p)))\n",
        "    print(f\"   Total: {total_params/1e6:.2f}M | Unique: {unique_params/1e6:.2f}M\")\n",
        "\n",
        "    # ---- Optimizer ----\n",
        "    optimizer = configure_optimizers(model, LR, weight_decay=0.1, device_type=\"cuda\")\n",
        "\n",
        "    num_update_steps_per_epoch = len(train_loader) // GRAD_ACCUM\n",
        "    total_steps = num_update_steps_per_epoch * EPOCHS\n",
        "    warmup_steps = int(total_steps * 0.1)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"\\nðŸš€ TRAINING (Run: {run_id})\")\n",
        "    print(f\"   Steps: {total_steps:,} | Warmup: {warmup_steps} | LR: {LR}\")\n",
        "\n",
        "    global_step = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for step, batch in enumerate(pbar):\n",
        "            x = batch['input_ids'].to(DEVICE)\n",
        "            y = batch['labels'].to(DEVICE)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1)) / GRAD_ACCUM\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % GRAD_ACCUM == 0:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                train_loss = loss.item() * GRAD_ACCUM\n",
        "                writer.add_scalar('Loss/Train', train_loss, global_step)\n",
        "                writer.add_scalar('GradNorm', grad_norm.item(), global_step)\n",
        "                writer.add_scalar('LR', scheduler.get_last_lr()[0], global_step)\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{train_loss:.4f}\",\n",
        "                    'gnorm': f\"{grad_norm.item():.4f}\",\n",
        "                    'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
        "                })\n",
        "\n",
        "            # Validation every 250 steps\n",
        "            if global_step > 0 and global_step % 250 == 0 and (step + 1) % GRAD_ACCUM == 0:\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                val_steps = 0\n",
        "                with torch.no_grad():\n",
        "                    for vb in valid_loader:\n",
        "                        vx = vb['input_ids'].to(DEVICE)\n",
        "                        vy = vb['labels'].to(DEVICE)\n",
        "                        val_loss += criterion(model(vx).view(-1, VOCAB_SIZE), vy.view(-1)).item()\n",
        "                        val_steps += 1\n",
        "                        if val_steps >= 500:\n",
        "                            break\n",
        "\n",
        "                avg_val = val_loss / val_steps\n",
        "                ppl = math.exp(avg_val) if avg_val < 20 else float('inf')\n",
        "                writer.add_scalar('Loss/Val', avg_val, global_step)\n",
        "                writer.add_scalar('PPL/Val', ppl, global_step)\n",
        "                print(f\"\\n   âœ¨ Step {global_step} | Val Loss: {avg_val:.4f} | PPL: {ppl:.2f}\")\n",
        "\n",
        "                if avg_val < best_loss:\n",
        "                    best_loss = avg_val\n",
        "                    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best.pt\"))\n",
        "                    print(f\"   ðŸ’¾ Saved best (loss={best_loss:.4f})\")\n",
        "\n",
        "                model.train()\n",
        "\n",
        "        # End-of-epoch checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_loss': best_loss,\n",
        "            'global_step': global_step,\n",
        "        }, os.path.join(SAVE_DIR, f\"epoch_{epoch+1}.pt\"))\n",
        "\n",
        "    # Final validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_steps = 0\n",
        "    with torch.no_grad():\n",
        "        for vb in valid_loader:\n",
        "            vx = vb['input_ids'].to(DEVICE)\n",
        "            vy = vb['labels'].to(DEVICE)\n",
        "            val_loss += criterion(model(vx).view(-1, VOCAB_SIZE), vy.view(-1)).item()\n",
        "            val_steps += 1\n",
        "\n",
        "    avg_val = val_loss / val_steps\n",
        "    ppl = math.exp(avg_val) if avg_val < 20 else float('inf')\n",
        "    print(f\"\\nðŸ“Š Final | Val Loss: {avg_val:.4f} | PPL: {ppl:.2f}\")\n",
        "\n",
        "    if avg_val < best_loss:\n",
        "        best_loss = avg_val\n",
        "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best.pt\"))\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"ðŸŽ‰ Done. Best Loss: {best_loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = run_training()\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    }
  ]
}